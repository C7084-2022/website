{"cells":[{"cell_type":"code","source":["%sh\n# Lab 00a: Before we begin, here's files that are counting against our QUOTA\n\ndu --human-readable --max-depth=1 --exclude='/dbfs' /"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70c6dcad-b049-4aa6-abb4-71ce48cc73a6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[" %sh\n # Lab 00c:\n    exec <&- 2> /dev/null\n    echo \"=Look for big s3 files:\"\n    du --human-readable --max-depth=2 --apparent-size --exclude='/dbfs/mnt' \\\n        --exclude='/dbfs/databricks-*' /dbfs\n    echo\n    echo \"=Look for big local files:\"\n    du --human-readable --max-depth=1 --exclude='/dbfs' /"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0823fc12-05a5-4268-82d0-3dccd07f5c37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\n# Lab 00d:\n\n# rm -rf /dbfs/FileStore/*.png\n# rm -rf /dbfs/tmp/*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06bd2ddc-88fc-4544-9ee6-f31319dbd6eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\n# Lab 00e:\n\n# rm -rf /dbfs/tmp/*\n# rm -rf /dbfs/local_disk0/tmp/*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0865e8df-566a-4500-b95e-edf35a5af32c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Mod 07: Spark Architecture and User Interface"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06e3edad-1faa-46c6-902f-ad6630a36c13"}}},{"cell_type":"code","source":["# Lab 00: First, disable side effects\nspark.conf.set(\"spark.databricks.io.cache.enabled\", False)\nspark.conf.set(\"spark.sql.adaptive.enabled\", False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4037baca-6b3c-4f36-bb72-f5516df6177e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 01: How Driver decides on Memory Partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"444f3a5b-8bc7-4379-8013-ce71a9d9fcb7"}}},{"cell_type":"code","source":["# Lab 01a: Here's the Data.  192 compressed files roughly 24.6 MB in size.  Total size on Disk = 4734 MB\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/wiki/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0916345a-91b6-4530-8584-786dfe638474"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: How many cores in Cluster?\nsc.defaultParallelism"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc141813-9c39-47c0-a0d3-2411effbceb6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: Max # of bytes to pack into a Partition (Default = 128 MB)\nspark.conf.get(\"spark.sql.files.maxPartitionBytes\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae91845b-1a4c-481f-bcbc-49a37fb9191f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: Overhead cost when packing files into a Partition (Default = 4 MB)\nspark.conf.get(\"spark.sql.files.openCostInBytes\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0222cfe9-621b-4ea2-b905-29587ee4e834"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01c:\n# Notice no Job spawn. That's because the Driver is solely responsible for building Partitions\nfrom pyspark.sql.types import LongType, TimestampType, StringType, StructType, StructField\n\nDDL_Schema = StructType([\n  StructField(\"id\", LongType(), True),\n  StructField(\"location\", StringType(), True),\n  StructField(\"ts\", TimestampType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"author\", StringType(), True)\n])\n\nwikiCSVDF = spark.read.option(\"sep\", \"\\t\").schema(DDL_Schema).csv(\"dbfs:/databricks-datasets/wiki/\")\nwikiCSVDF.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47c661e2-937d-4971-a68d-7ac1b97a7f74"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01d: repartition() increases # of Partitions\n# This is used to get Partition size between 200MB - 1GB range\nwikiDF16 = wikiCSVDF.repartition(16)\nwikiDF16.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0b4a620-19a2-4e36-910b-df781eead957"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01e: coalesce() decreases # of Partitions\n# This is used to get Partition size between 200MB - 1GB range\nwikiDF04 = wikiDF16.coalesce(4)\nwikiDF04.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b275cd52-d731-4472-99b2-5155883b73b0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 02:  Spark UI walkthrough"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bd998be-0891-4add-a69c-a810d80063d0"}}},{"cell_type":"code","source":["# Lab 02a: First create some DataFrames and TempViews\nempDF = spark.read.parquet(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\ndeptDF = spark.read.parquet(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\n\nempDF.createOrReplaceTempView(\"emp_view\")\ndeptDF.createOrReplaceTempView(\"dept_view\")\n\ndisplay(empDF)\ndisplay(deptDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd30c34e-31e3-4494-9620-6c3fc67c344b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 02b: Am dropping Hint to Catalyst Optimizer to force Sort Merge Join\n-- Follow along with Instructor to see what Spark UI has to say about this query\nSELECT /*+ SHUFFLE_MERGE(dept_view) */ * FROM emp_view JOIN dept_view ON emp_view.dept = dept_view.dept"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec70cbc9-d99e-4dd3-acde-6be3582b7201"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 03: 'inferSchema' spawns 2 Jobs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7faf8014-874e-45cb-8c71-881707c50b1d"}}},{"cell_type":"code","source":["# Lab 03a: Note 'inferSchema' causes 2 Jobs to run (1 to read first row of file to see if Column names defined, and 1 to infer Data Types)\n#          So best practice is to always hard-code Schema to avoid this overhead (as seen in Cell 18)\ndf = (spark.read.option(\"header\", True).option(\"inferSchema\", True)\n          .csv(\"dbfs:/databricks-datasets/asa/airlines/2007.csv\"))\ndf.createOrReplaceTempView(\"air_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c09af386-d6e8-4427-8038-81ce87335c41"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 03b: Here's Schema we'll use in next Cell\nDDL_Schema = (\"Year integer,Month integer,DayofMonth integer,DayOfWeek integer,DepTime string,CRSDepTime integer,ArrTime string,CRSArrTime integer,UniqueCarrier string,FlightNum integer,TailNum string,ActualElapsedTime string,CRSElapsedTime integer,AirTime string,ArrDelay string,DepDelay integer,Origin string,Dest string,Distance integer,TaxiIn integer,TaxiOut integer,Cancelled integer,CancellationCode string,Diverted integer,CarrierDelay string,WeatherDelay string,NASDelay string,SecurityDelay string,LateAircraftDelay string\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75b0033a-16cd-4c21-863f-f0364799e572"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 03c: No Job(s) spawned here since Schema defined\ndf = spark.read.option(\"header\", True).schema(DDL_Schema).csv(\"dbfs:/databricks-datasets/asa/airlines/2007.csv\")\ndf.createOrReplaceTempView(\"flights_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa021791-d0f3-49bd-aaf0-67a1db02bc48"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 04: Narrow versus Wide Tasks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e186d596-9b7a-4c87-bc79-a763d8d2fa76"}}},{"cell_type":"code","source":["# Lab 04a: 'select' and 'filter' are Narrow Task since they stay in same Stage\ndisplay(df.select(\"UniqueCarrier\", \"Distance\").filter(\"Distance > 2000\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15a77205-5b33-4aad-abdf-098020bc4390"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 04b: Is an 'orderBy' a Wide or Narrow Task? \ndisplay(df.select(\"UniqueCarrier\", \"Distance\").filter(\"Distance > 4600\").orderBy(\"Distance\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21b4b341-81d0-4042-b051-8293056377fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 04c: Is 'union' a Wide Task? \ndisplay(df.union(df).limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12b06bdb-3627-4fea-8ea0-e34472e75617"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 05: Wide Tasks spawn new Stage (along with mandatory Shuffle/Exchange)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"282fe0d6-a8b8-4352-87cd-d2c88c187cb2"}}},{"cell_type":"code","source":["# Lab 05a: Let's create 2 new objects so we can Join on them in below Cells\nempDF = spark.read.parquet(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\ndeptDF = spark.read.parquet(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\n\nempDF.createOrReplaceTempView(\"emp_view\")\ndeptDF.createOrReplaceTempView(\"dept_view\")\n\ndisplay(empDF)\ndisplay(deptDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23571218-6bb4-4cab-b0b0-fadbe4cf0666"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 05b: Am dropping Hint to Catalyst Optimizer to force Sort Merge Join\nSELECT /*+ SHUFFLE_MERGE(dept_view) */ * FROM emp_view JOIN dept_view ON emp_view.dept = dept_view.dept"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f051056b-4461-4478-a607-aca41ac24834"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 05c: Aggregations are Wide Tasks too\nSELECT dept, sum(salary) FROM emp_view\nGROUP BY dept;\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"213d28ad-69bc-43e2-b16a-ed315182b820"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 05d: Distinct is Wide Task too\nSELECT distinct dept FROM emp_view;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86b6f82d-7c93-47e8-8d95-fe3a8ba11cc6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 06: In Review\n## Remove 'Comment' before running each Cell"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c2832a2-6288-4df6-8324-e6f595bf23c5"}}},{"cell_type":"code","source":["%sql\n--  Lab 06a: Shuffle?\n-- SELECT * FROM emp_view INTERSECT SELECT * FROM emp_view"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11910b21-24de-4a25-bf56-6afab4d6ac64"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 06b: Shuffle?\n-- CREATE TABLE emp_table AS SELECT * FROM emp_view"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"095f8401-3096-46c4-a48f-5b0ee9d9ce8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 06c: Shuffle?\n#flyDF = spark.read.format(\"csv\").load(\"dbfs:/FileStore/tables/header_flights_abbr.csv\", inferSchema=\"true\", header=\"true\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42219b79-88b1-4122-952b-ff665886c8d5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 06d: Shuffle?\n-- SELECT * FROM emp_view WHERE NOT dept IS NOT NULL OR emp > 1000 LIMIT 20"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f68a3385-6f19-4304-8bab-c31578ba176b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 06e: Shuffle?\n# display(wikiDF04.repartition(24))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e81fb64-c8ef-47b6-9c40-ef386db1caa8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Mod 07 - Architecture - Spark UI\n## Ignore past here"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c71d454-7e5f-46a4-885f-5ce376e98c17"}}},{"cell_type":"code","source":["%scala\n// Lab 03b: Notice how easy it is to read DAG (from top to bottom) with Scala\n// Create EMP and DEPT rdd\n// Since these RDDs are not dependent on each other, they may run in parallel\n//val emp = sc.parallelize(Seq((1, \"mark\", 10), (2, \"juli\", 20), (3, \"carol\", 30), (4, \"jarrod\", 35), (5, \"karen\", 30)), 80)\n//val dept = sc.parallelize(Seq((\"hadoop\", 10), (\"spark\", 20), (\"hive\", 30), (\"sqoop\", 40) ), 40)\n\n// Establish 3rd field as Key/Join column for EMP rdd, 2nd columm for DEPT rdd\n//val emp1 = emp.keyBy(col => col._3)\n//val dept1 = dept.keyBy(col => col._2)\n\n// Inner Join\n//val join1 = emp1.join(dept1)\n//join1.toDebugString"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8df2d2f3-c75f-41e3-9fa8-01fbbd52d8e7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 03c: Notice how easy it is to read DAG (from top to bottom) with Scala\n// Create EMP and DEPT rdd\n// Since these RDDs are not dependent on each other, they may run in parallel\n//val emp = sc.parallelize(Seq((1, \"mark\", 10), (2, \"juli\", 20), (3, \"carol\", 30), (4, \"jarrod\", 35), (5, \"karen\", 30)), 80)\n//val dept = sc.parallelize(Seq((\"hadoop\", 10), (\"spark\", 20), (\"hive\", 30), (\"sqoop\", 40) ), 40)\n\n// Establish 3rd field as Key/Join column for EMP rdd, 2nd columm for DEPT rdd\n//val emp1 = emp.keyBy(col => col._3)\n//val dept1 = dept.keyBy(col => col._2)\n\n// Inner Join\n//val join1 = emp1.join(dept1)\n//join1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"165d8f85-5df5-446d-bd30-df5808cdff8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%py\n# Lab 03d:  Notice diffrence DAG with Python\n# Create EMP and DEPT rdd\n# Since these RDDs are not dependent on each other, they may run in parallel\n#emp = sc.parallelize([ (1,\"mark\",10), (2,\"juli\",20), (3,\"matt\",30), (4,\"jay\",35), (5,\"sue\",30) ])\n#dept = sc.parallelize([ (\"hadoop\", 10), (\"spark\", 20), (\"hive\", 30), (\"sqoop\",40) ])\n\n# Establish 3rd field as Key/Join column for EMP rdd, 2nd columm for DEPT rdd\n#emp1 = emp.keyBy(lambda j: j[2] )\n#dept1 = dept.keyBy(lambda j: j[1] )\n\n# Inner Join\n#join1 = emp1.join(dept1)\n#join1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c007f720-2bec-4b54-af31-8a6afc66aee4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 04: Storage tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad53e57b-16f2-41f3-9000-62c45f11e256"}}},{"cell_type":"code","source":["%py\n# Lab 04a: Cache a DataFrame\n\n#df1 = spark.read.format(\"json\").load(\"dbfs:/FileStore/tables/names1.json\")\n\n#df1.cache()\n#df1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab24bed2-1ad9-43ad-b4fd-40fd07fea00f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 04b: Cache a Table\n\n#DROP TABLE IF EXISTS dept;\n\n#CREATE TABLE dept (dept_num INT, dept_name STRING, budget INT, mgr INT)\n#USING csv\n#OPTIONS (path \"dbfs:/FileStore/tables/dept.csv\");\n\n#CACHE TABLE dept;\n#SELECT * FROM dept;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c49a1c1f-c631-4293-84a0-06e10927dbd3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 04c: Cache an RDD\n\n//val emp = sc.parallelize(Seq((1, \"mark\", 10), (2, \"juli\", 20), (3, \"carol\", 30), (4, \"jarrod\", 35), (5, \"karen\", 30)), 80)\n//val dept = sc.parallelize(Seq((\"hadoop\", 10), (\"spark\", 20), (\"hive\", 30), (\"sqoop\", 40) ), 40)\n\n// Establish 3rd field as Key/Join column for EMP rdd, 2nd columm for DEPT rdd\n//val emp1 = emp.keyBy(col => col._3)\n//val dept1 = dept.keyBy(col => col._2)\n\n// Assign RDD name and then Cache\n//emp1.setName(\"empRDD\").cache()\n//dept1.setName(\"deptRDD\").cache()\n\n// Inner Join\n//val join1 = emp1.join(dept1)\n//join1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799fa961-e808-4c6a-af4f-b3fb3da3c984"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 04d:  Run JOIN again.  Since it is now cached, can skip some Stages\n//join1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aeb849d6-98ae-4522-9400-6c4d73d9bc99"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 05: Environment tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30f688ab-68b2-4bd0-9ada-eb7f3dc38535"}}},{"cell_type":"code","source":["%scala\n// Lab 05:  Run JOIN again. 'View' for 'Environment' tab\n//join1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f9e759d-5f58-466b-9252-1ebb5632dd7b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 06: Executors (and Drivers) tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fe155cd-23d5-400e-bd22-6815fca70c5a"}}},{"cell_type":"code","source":["%py\n#sc.defaultParallelism"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"509cd378-880d-4939-a1e4-7219edd0263e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 06:  Run JOIN again. 'View' for 'Executor' tab\n//join1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c4eb7e2-c9b2-422c-a2da-276cc481038d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 07: SQL tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c1728b4-0ef3-4478-af2c-6650d08a4304"}}},{"cell_type":"code","source":["%scala\n// Lab 07a: View 'sql' tab. Then click 'Details' hotlink\n//val df = Seq((1, \"andy\"), (2, \"bob\"), (2, \"andy\")).toDF(\"count\", \"name\")\n\n\n//df.count                                                           \n//df.createOrReplaceTempView(\"temp_view1\")\n\n//spark.sql(\"select name,sum(count) from temp_view1 group by name\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bd146e5-dd3b-4c3f-938b-1b8349e4bd7d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%py\n# Mod 07b: Fist Load 2 Parquet files and JOIN the DataFrames\n\n#empDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\n#deptDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\n#joinDF = empDF.join(deptDF, \"dept\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f1f4389-5ab6-4fd1-bdb6-aabefad46d2e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%py\n# Mod 07c: Run query and then view 'sql' tab.  Then click 'Details' hotlink\n\n#display(joinDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7c4d86b-b8f5-4e99-a2e2-b9ee6e63e204"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 08: JDBC/ODBC Server tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01d30ee6-ce3a-4b65-be54-ef7e66fd6afd"}}},{"cell_type":"code","source":["%scala\n// Spark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface. In this mode, end-users or applications can interact with Spark // SQL directly to run SQL queries, without the need to write any code.\n// For example, could query Hive table usin the Spark Engine.\n// This functionality is not configurated for Databrick Community"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b7acc93-cdc8-431e-bb5b-e986afea32be"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 09: Structured Streaming tab"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77741462-1722-4064-90b0-0fbef38db32c"}}},{"cell_type":"code","source":["%python\n\n# Lab 09a: Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n# Since the sample data is just a static set of files, you can emulate a stream from them by reading one #file at a time, in the chronological order in which they were created\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ninputPath = \"/databricks-datasets/structured-streaming/events/\"\n\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\n# Static DataFrame representing data in the JSON files\nstreamingInputDF = (\n  spark\n    .readStream\n    .schema(jsonSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aed4865-373e-49b6-a703-c4f946330ff8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n\n# Lab 09b: Now we can compute the number of \"open\" and \"close\" actions with one minute windows. \n#To do this, we will group by the action column and 1 minute #windows #over the time column. Same query as staticInputDF earlier\n\nstreamingCountsDF = (\n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action,\n      window(streamingInputDF.time, \"1 minute\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11191402-4dac-432d-9db1-240f82575cf4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%py \n\n# Lab 9c: Once started, click on 'Spark Jobs' and then click on 'View' hotlink\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ce7f302-23e0-49c3-ab21-339d3211df56"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%py\n# Lab 9d: Stop Streaming\n\nquery.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f0120cb-276a-41aa-abf8-fc0b23786f82"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Module 08: Architecture and Spark UI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57fea17a-2155-4546-ba98-c0c12019d7e3"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mod-07-Arch-UI!","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3262131266600858}},"nbformat":4,"nbformat_minor":0}
