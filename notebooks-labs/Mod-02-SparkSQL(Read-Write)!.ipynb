{"cells":[{"cell_type":"markdown","source":["## Mod 02: Spark SQL (Read/Write DataFrames/Tables)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f797d59-ff9f-42e1-b99b-060bb5feb30b"}}},{"cell_type":"markdown","source":["### Lab 01: What is a DataFrame? Object with a Schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c6994a4-7f42-4e5a-ba35-7ccf41d24a67"}}},{"cell_type":"code","source":["# Place cursor after last '.' and click TAB key to see which file types are supported\nspark.read."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e751f883-73fd-4128-99bf-0e1422b7499f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#  Lab 01a: Using 'spark.read' to create a DataFrame using CSV file format\ndf1 = spark.read.load(\"dbfs:/FileStore/tables/LifeExp_headers.csv\", format=\"csv\", header= True, inferSchema= True )\ndisplay(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f044335-5eb7-4b93-bc6d-d50b587cbf3c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: Alternative code\ndf1 = spark.read.option(\"header\", True).option(\"inferSchema\", True).format(\"csv\").load(\"dbfs:/FileStore/tables/LifeExp_headers.csv\")\ndisplay(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f87251c-500c-41f8-b143-c7e9e44aded9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#  Lab 01c: Columns Implicit via hard-coded default column name = '_cX'\n#           (where X is Integer starting at 0 and incrementing by 1)\ndf2 = spark.read.load(\"dbfs:/FileStore/tables/LifeExp.csv\", format=\"csv\", header=\"False\", inferSchema=\"True\")\ndisplay(df2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e6c97d8-2f16-4814-962f-d0fe42671771"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 02: Create Dataframe from JSON file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27cda62f-f36d-4b9e-8ff5-20e88da911a3"}}},{"cell_type":"code","source":["# Lab 02a: Using 'spark.read' function, load JSON file, then display\n\ndf1 = spark.read.format(\"json\").option(\"inferSchema\", True).load(\"dbfs:/FileStore/tables/names1.json\")\ndisplay(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70720767-1f43-49ae-9917-2afe83df4635"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02b: View schema\n\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfe14d44-7482-4bda-bfd6-94560232bec3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02c: Query Dataframe using 'select' with 'display'\n\n# Below 2 equivalent answer set\ndisplay(df1.select(\"name\"))\ndisplay(df1.select(df1.name))\n\ndisplay(df1.select(df1[\"name\"], df1[\"age\"]))\ndisplay(df1[df1.age<50])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fa77b58-0adb-4613-87a3-a6d604fbb184"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 03: Read Parquet files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"326fd293-456f-49ab-8d7d-e94b34f4c513"}}},{"cell_type":"code","source":["# Load Parquet files and display\n# Parquet file format has Schema (Column names and Data types built-in the Metadata)\n\nempDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\ndeptDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\n\ndisplay(empDF)\ndisplay(deptDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a8a33cb-7440-4950-9137-bf7311714f17"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 04a: spark.read.csv with StructType Libraray and format and schema arguments"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"622f52d5-a440-437a-9f3a-2253dd6b30eb"}}},{"cell_type":"code","source":["## Lab 04a: How to manually define Schema for your CSV file\n## Import library so can create Schema\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n\n## Create schema to be used in creation of DataFrame\nlifeSchema = StructType([StructField('Country',StringType(), True), \\\n                         StructField('LifeExp', FloatType(), True), StructField('Region', StringType(), True) ])\n\ndf1 = spark.read.load(\"dbfs:/FileStore/tables/LifeExp.csv\", format=\"csv\", schema=lifeSchema)\n\ndisplay(df1)\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d490ef52-8943-4c1f-aca1-e16a607d9581"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 04b: Even easier than StructType, just code Schema like  below"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6212ec19-adeb-4b24-ae03-7753d55bf5ee"}}},{"cell_type":"code","source":["# With Spark version 3, don't even have to mess around with 'StructType' anymore!!  Just do it like below\nDDLSchema = \"Country string, LifeExp float, Region string\"\n\ndf1 = spark.read.format(\"csv\").load(\"dbfs:/FileStore/tables/LifeExp.csv\", schema=DDLSchema)\n\ndisplay(df1)\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e098e0d3-ddf3-432e-a338-6e07bc8af34d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 05: Using PostgreSQL database"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dc889f3-54f6-4c98-9650-4bfbb6299e2c"}}},{"cell_type":"code","source":["%scala\n// Here's JDBC driver and credentials to log into PostgreSQL\nimport org.apache.spark.sql.functions._\n\n// Configure this app to connect to a PostgreSQL database\nClass.forName(\"org.postgresql.Driver\")\n\nval connectionProperties = new java.util.Properties()\nconnectionProperties.put(\"user\", \"readonly\")\nconnectionProperties.put(\"password\", \"readonly\")\n\nval tableName = \"training.people_1m\"\nval jdbcUrl = \"jdbc:postgresql://server1.databricks.training:5432/training\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b23f66df-57be-4f8e-ae19-774e581e622d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Here we query PostgreSQL table \nsc.setJobDescription(\"Step C: With Predicate\")\n\nval df = spark.read\n  .jdbc(url=jdbcUrl, table=tableName, properties=connectionProperties) // Open a JDBC connect\n  .filter($\"id\" > 343517)                                             \ndisplay(df) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7118c38b-d05f-4bca-a814-25dec24133d0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 06: Write DataFrame to Files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"446b2d3a-01af-4e52-ae8e-aecc1625d0ef"}}},{"cell_type":"code","source":["# Lab 06a: Write as Delta file\nempDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\nempDF.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dcc2536-5a5f-407e-aa58-226468ff4f14"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# View the files\ndisplay(dbutils.fs.ls(\"tmp/delta/\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68673e4e-27b9-433e-8bb3-666b71a3bcf3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 06b: Save as an SQL Table\n# Can view table via the 'Data' menu\n# empDF.write.format(\"delta\").mode(\"append\").saveAsTable(\"emp_table\")\n\n# Note this lab will fail if you run again.  To fix, uncomment below and then run.  Then put comment back\n#spark.sql(\"DROP TABLE IF EXISTS emp_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9bd4a13-d3bd-452f-9c9a-a9f403dd2609"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 07: Write as JSON, ORC and Parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0289a62-d145-4d42-8fce-20307eba55bd"}}},{"cell_type":"code","source":["# Write empDF in other File formats\nempDF.select(\"emp\", \"mgr\", \"dept\").write.format(\"json\").mode(\"overwrite\").save(\"tmp/JSON/\")\nempDF.select(\"emp\", \"mgr\", \"dept\").write.format(\"orc\").mode(\"overwrite\").save(\"tmp/orc/\")\nempDF.select(\"emp\", \"mgr\", \"dept\").write.format(\"parquet\").mode(\"overwrite\").save(\"tmp/parquet/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b03bb876-0a1a-493b-a4f6-402db9c0a46a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Which one has least disk space?\ndisplay(dbutils.fs.ls(\"tmp/JSON/\"))\ndisplay(dbutils.fs.ls(\"tmp/orc/\"))\ndisplay(dbutils.fs.ls(\"tmp/parquet/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"892d5704-a4f8-4d2b-8578-1b0e4c7e49b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 08: SHOW TABLES displays SQL Tables and TempViews"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e51583eb-e3e3-48a1-b717-5e34b3277de3"}}},{"cell_type":"code","source":["%sql\n-- Here's the table we created in Cmd Cell 23\n-- 'isTemporary' = True earmarks any TempViews you may have created via 'CreateOrReplaceTempView' statement\nshow tables;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bffd0895-8a14-49fd-bd0d-db5614676853"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Alternative code: If wish to run in SQL-like code in Python, just preface with 'spark.sql'\ndisplay(spark.sql(\"show tables\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58dd4d9f-a3be-49ae-b967-dff2c3e1d22b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 09: Create Table: USING and OPTIONS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"effbff19-055c-4fbd-be3a-ed15adcccc3e"}}},{"cell_type":"code","source":["%sql\n-- Lab 12a: DROP, then CREATE TABLE\n-- Note if PATH points to files, Table is instantly populated\nDROP TABLE IF EXISTS mpg;\n\nCREATE TABLE mpg\nUSING csv\nOPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/mpg.csv\", header \"true\");"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74c57ce1-2593-40d9-bbc6-d98c7696941c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Query Table\n\nSELECT * FROM mpg;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0acb921-feab-4150-86a3-2feafdc90670"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 10: Follow along with Instructor as View Table details via 'Data' tab in UI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94abf5ae-196d-4e16-825a-fb7cb87c10f4"}}},{"cell_type":"markdown","source":["### Lab 11: Create Table without Header info by creating Column names/data types manually"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60e67d5b-407b-49e7-9494-463870b1ce51"}}},{"cell_type":"code","source":["%sql\n-- Lab 11a: Create Table (Here I'm using 'LOCATION' keyword instead of 'PATH' keyword)\nDROP TABLE IF EXISTS dept;\n\nCREATE TABLE dept (dept_num INT, dept_name STRING, budget INT, mgr INT)\nUSING csv\nLOCATION \"dbfs:/FileStore/tables/dept.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6290827e-be42-4e8f-a30f-1da4666e1437"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 11b: View Table contents\n\nSELECT * FROM dept;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64ca2fb1-97e5-4bad-a154-a0fad26d0c72"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 11c: Create DataFrame from Hive Table using 'spark.table'\ndf = spark.table(\"dept\")\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f819800a-67fe-4635-97d6-1079089b2546"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 12: Create Table: PARTITION BY"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3607d59e-6352-4432-928a-93bb21d9b627"}}},{"cell_type":"code","source":["# Uncomment this and run if wish to run next Cell again\n# dbutils.fs.rm('/user/hive/warehouse/cust_part', True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e072755-a135-4bae-a9a5-8052ea665e4b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 12a: \nDROP TABLE IF EXISTS cust_part;\n\nCREATE TABLE cust_part (id INT, name STRING) \nUSING DELTA\nPARTITIONED BY (state STRING, city STRING); "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abc3bcf3-8f4f-499a-a486-886b779a756b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 12b: INSERT data into Table\nINSERT INTO cust_part PARTITION (state = 'CA', city = 'Fremont') VALUES (100, 'Al'); \nINSERT INTO cust_part PARTITION (state = 'CA', city = 'San Jose') VALUES (200, 'Bo'); \nINSERT INTO cust_part PARTITION (state = 'AZ', city = 'Peoria') VALUES (300, 'Cy'); "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d785c47-e592-4a92-92c8-5d5401485974"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 12c: View Table contents\nSELECT * FROM cust_part;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b341c28c-de09-4e9b-ad46-19ee8f7d8d16"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 12d: Details about the Table\nDESCRIBE EXTENDED cust_part;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df21539b-b75e-47df-9f85-4c457aa19693"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 12e: Confirm only reading 1 Partition (CA). Look for 'PartitionFilters'\n\nEXPLAIN FORMATTED SELECT * FROM cust_part WHERE state = 'CA';"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e5e9f1c-8822-44ef-9df7-60333e754e07"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 12f: Confirm have 2 Directories under dbfs:/user/hive/warehouse/cust_part\n\ndisplay(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/cust_part/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a16045c7-6942-4774-b4b4-bae486b97288"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 12g: Drill down to state=CA partition and notice 2 CITY partitions under it\n\ndisplay(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/cust_part/state=CA\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c184bf4e-4c3f-4e2c-868c-3f4a85a0fe48"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Drill down to 'city=Fremont' Partition to view actual File(s)\n# Lab 12h:\n\ndisplay(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/cust_part/state=CA/city=Fremont/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b9c1043-531e-4b53-b54d-57e27488a6db"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 13: CREATE BUCKET TABLE: Clustered By Sorted By INTO Num_Buckets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3470c41c-329e-4108-a2b1-9d4ec1415967"}}},{"cell_type":"code","source":["# Uncomment this and run if wish to run next Cell again\n# dbutils.fs.rm('/user/hive/warehouse/bucket_table', True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88dfa541-7fe1-4dae-9d78-4eea6efce60d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 13a: Create 25 buckets (files) and has 'State' values to one of these Buckets\n--          Note it is possible to get Skewed files this way\n\nDROP TABLE IF EXISTS bucket_table;\n\nCREATE TABLE bucket_table (state STRING, population INT, yr INT)\n        USING CSV\n        COMMENT 'A bucketed sorted user table'\n        CLUSTERED BY (state) SORTED BY (state) INTO 25 BUCKETS;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70510633-9b68-4b3b-aabc-5789e34a343e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 13b: INSERT\n\nINSERT OVERWRITE bucket_table VALUES\n(\"Alabama\",4875120,2017),\n(\"Alaska\",739786,2017),\n(\"Arizona\",7048876,2017),\n(\"Arkansas\",3002997,2017),\n(\"California\",39399349,2017),\n(\"Colorado\",5615902,2017),\n(\"Connecticut\",3573880,2017),\n(\"Delaware\",957078,2017),\n(\"DistrictofColumbia\",695691,2017),\n(\"Florida\",20976812,2017),\n(\"Georgia\",10413055,2017),\n(\"Hawaii\",1424203,2017),\n(\"Idaho\",1718904,2017),\n(\"Illinois\",12786196,2017),\n(\"Indiana\",6660082,2017),\n(\"Iowa\",3143637,2017),\n(\"Kansas\",2910689,2017),\n(\"Kentucky\",4453874,2017),\n(\"Louisiana\",4670818,2017),\n(\"Maine\",1335063,2017),\n(\"Maryland\",6024891,2017),\n(\"Massachusetts\",6863246,2017),\n(\"Michigan\",9976447,2017),\n(\"Minnesota\",5568155,2017),\n(\"Mississippi\",2989663,2017),\n(\"Missouri\",6108612,2017),\n(\"Montana\",1053090,2017),\n(\"Nebraska\",1917575,2017),\n(\"Nevada\",2972405,2017),\n(\"New Hampshire\",1349767,2017),\n(\"New Jersey\",8888543,2017),\n(\"New Mexico\",2093395,2017),\n(\"New York\",19590719,2017),\n(\"North Carolina\",10270800,2017),\n(\"North Dakota\",755176,2017),\n(\"Ohio\",11664129,2017),\n(\"Oklahoma\",3932640,2017),\n(\"Oregon\",4146592,2017),\n(\"Pennsylvania\",12790447,2017),\n(\"Rhode Island\",1056486,2017),\n(\"South Carolina\",5021219,2017),\n(\"South Dakota\",873286,2017),\n(\"Tennessee\",6708794,2017),\n(\"Texas\",28322717,2017),\n(\"Utah\",3103118,2017),\n(\"Vermont\",624525,2017),\n(\"Virginia\",8465207,2017),\n(\"Washington\",7425432,2017),\n(\"West Virginia\",1817048,2017),\n(\"Wisconsin\",5792051,2017),\n(\"Wyoming\",578934,2017),\n(\"Alabama\",4887871,2018),\n(\"Alaska\",737438,2018),\n(\"Arizona\",7171646,2018),\n(\"Arkansas\",3013825,2018),\n(\"California\",39557045,2018),\n(\"Colorado\",5695564,2018),\n(\"Connecticut\",3572665,2018),\n(\"Delaware\",967171,2018),\n(\"DistrictofColumbia\",702455,2018),\n(\"Florida\",21299325,2018),\n(\"Georgia\",10519475,2018),\n(\"Hawaii\",1420491,2018),\n(\"Idaho\",1754208,2018),\n(\"Illinois\",12741080,2018),\n(\"Indiana\",6691878,2018),\n(\"Iowa\",3156145,2018),\n(\"Kansas\",2911505,2018),\n(\"Kentucky\",4468402,2018),\n(\"Louisiana\",4659978,2018),\n(\"Maine\",1338404,2018),\n(\"Maryland\",6042718,2018),\n(\"Massachusetts\",6902149,2018),\n(\"Michigan\",9995915,2018),\n(\"Minnesota\",5611179,2018),\n(\"Mississippi\",2986530,2018),\n(\"Missouri\",6126452,2018),\n(\"Montana\",1062305,2018),\n(\"Nebraska\",1929268,2018),\n(\"Nevada\",3034392,2018),\n(\"New Hampshire\",1356458,2018),\n(\"New Jersey\",8908520,2018),\n(\"New Mexico\",2095428,2018),\n(\"New York\",19542209,2018),\n(\"North Carolina\",10383620,2018),\n(\"North Dakota\",760077,2018),\n(\"Ohio\",11689442,2018),\n(\"Oklahoma\",3943079,2018),\n(\"Oregon\",4190713,2018),\n(\"Pennsylvania\",12807060,2018),\n(\"Rhode Island\",1057315,2018),\n(\"South Carolina\",5084127,2018),\n(\"South Dakota\",882235,2018),\n(\"Tennessee\",6770010,2018),\n(\"Texas\",28701845,2018),\n(\"Utah\",3161105,2018),\n(\"Vermont\",626299,2018),\n(\"Virginia\",8517685,2018),\n(\"Washington\",7535591,2018),\n(\"West Virginia\",1805832,2018),\n(\"Wisconsin\",5813568,2018),\n(\"Wyoming\",577737,2018)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a6db9f6-2b6d-4647-af5c-9d2fccd543ab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 13c: Confirm only reading 1 Bucket. Look for Number of Buckets Selected\n\nEXPLAIN FORMATTED SELECT * FROM bucket_table WHERE state = 'Ohio';"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e2596b3-0e57-4d20-b136-15341dc0328d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 13d: View DDL of Table\nSHOW CREATE TABLE bucket_table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5211c62f-fcf9-4dbf-8f0d-149a13c0459c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 14: CREATE TABLE: TBLPROPERTIES \n### Creates Key-Values pairs that can be viewed via 'SHOW TBLPROPERTIES' query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3d75915-594b-4489-a429-665efa0a6043"}}},{"cell_type":"code","source":["%sql\n-- Lab 14a:\n\nDROP TABLE IF EXISTS customer1;\n\nCREATE TABLE customer1(cust_code INT, name VARCHAR(100), cust_addr STRING)\n  TBLPROPERTIES ('created.by.user' = 'Mark', 'created.date' = '01-01-2021');"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cce4f517-4e77-4fe6-86b0-456ecadbe2e8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 14b: Comments show up in this command\n\nSHOW TBLPROPERTIES customer1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb1fb5aa-2e34-4d94-8fad-528f1d4f3b46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 14c:  Comments show up in this command too\nDESCRIBE EXTENDED customer1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6359acb-8454-4a6d-b951-edf2d2ad96ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 15: Creating TempViews via 'CreateOrReplaceTempView'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7a2f003-a18d-45bd-aaee-751ecf68f24b"}}},{"cell_type":"code","source":["# Lab 15a: Convert DataFrame into a TempView\n\ndeptDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\ndeptDF.createOrReplaceTempView(\"dept_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"258de1a0-7d3e-4d09-8276-9e523abb5e4d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 15b: show tables. Note have both PERM and TEMP objects\n\nshow tables;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15483f54-b5b6-409b-92fe-1e6c4a781ddf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 15c: Query Temp View\n\nSELECT * FROM dept_view;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb225351-fbfa-4974-a0e8-fffcbf689134"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 15d: Saving a TempView as a Perm Table\nCREATE TABLE perm_dept_table2 AS SELECT * FROM dept_view;\n\nSHOW TABLES;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1102bcae-c3b4-43b4-af47-9fccfcc1ee6a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 15e: Delete Cluster, Add Cluster back and repeat Cmd 60.\n### Confirm 'dept_view' is no longer there"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6f30f84-b4a4-468a-bada-27b2e77b2219"}}},{"cell_type":"markdown","source":["### Lab 16: Global Temporary Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ca985ef-b4cc-4404-bdec-bf75543b1d33"}}},{"cell_type":"code","source":["# Uncomment below and run if wish to run next Cell again\n# spark.catalog.dropGlobalTempView(\"people\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3aaec47-4bcb-46a1-a38c-280242a74fdd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 16a:  Run Global Temp View across 2 sessions: They both work\n# Note if need to drop a Global Temp View, use the code in above Cell (commented out since don't need it for lab)\n# spark.catalog.dropGlobalTempView(\"people\")\n\ndf = spark.read.json(\"dbfs:/FileStore/tables/names1.json\") \ndf.createGlobalTempView(\"people\") \n\n\n# Global temporary view is tied to a system preserved database `global_temp`\nspark.sql(\"SELECT * FROM global_temp.people\").show() \n\n# Global temporary view successful in another Session\nspark.newSession().sql(\"SELECT * FROM global_temp.people\").show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89b7a50a-3d24-46e1-846b-a535143e4462"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 16b:  Run Temp View across 2 sessions: 2nd Session fails on Purpose since it's a Temp View.\n#           It ran in previous lab because it was a Global View\ndf1 = spark.read.json(\"dbfs:/FileStore/tables/names1.json\") \ndf1.createOrReplaceTempView(\"people_temp_view\")\n\n# Now Convert TempView to a DataFrame using following syntax\nspark.sql(\"SELECT * from people_temp_view\").show()\n\n\n# Temporary views ARE NOT cross-session. Query Fails intentionally\nspark.newSession().sql(\"SELECT * FROM people_temp_view\").show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e15425a-82b0-4a41-bea8-3688b74c7725"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Module 02 - SparkSQL (Read/Write DataFrames/Tables)\n## Ignore past here"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99bee31d-d7a6-40e3-b4ed-62645458ea89"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mod-02-SparkSQL(Read-Write)!","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3262131266600789}},"nbformat":4,"nbformat_minor":0}
