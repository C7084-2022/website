{"cells":[{"cell_type":"code","source":["# If need to repeat labs, may have to run this first\n# display(dbutils.fs.rm(\"dbfs:/user/hive/warehouse/\", True))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7223687b-0703-40b5-adc2-31ef402eacb4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Mod 08: Catalog, Catalyst, Tungsten"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4cfd105-be98-4474-98e8-09e3484b589a"}}},{"cell_type":"markdown","source":["## Lab 01: Catalog - List Hive Databases, Tables, TempViews"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c889e95-5fd5-4258-a5e3-a40c669220ad"}}},{"cell_type":"code","source":["%scala\n// Lab 01a: List Databases\n\ndisplay(spark.catalog.listDatabases)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4160e0d1-a437-4792-acb7-d3319976a298"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: List Databases (python)\ndisplay(spark.catalog.listDatabases())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6009f97a-076e-4fdc-bb94-dc2f76dbd498"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 01c: Create TempView, then view in HCAT Metadata Catalog\nval df = Seq((1, \"andy\"), (2, \"bob\"), (2, \"andy\")).toDF(\"count\", \"name\")                                                    \ndf.createOrReplaceTempView(\"temp_view1\")\ndisplay(spark.catalog.listTables)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4f2d600-21bf-49d8-b414-8d19edbdc65f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls(\"dbfs:/user/hive/warehouse/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf2b0c61-5fa1-4eb1-9247-f1ae1bcd3981"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 01c: Create TempView, then view in HCAT Metadata Catalog\nCREATE TABLE IF NOT EXISTS perm_mng_table1 AS SELECT * FROM temp_view1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d445b5d9-bb30-4bac-9c31-8ac9daf1681c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab\nCREATE TABLE IF NOT EXISTS perm_ext_table1 LOCATION '/tmp/ext2/' AS SELECT * FROM temp_view1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ba2c2b9-e630-4c38-8abd-1a9de9e0e1c7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 01d: \ndisplay(spark.catalog.listTables)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adad393e-6e00-4de6-92db-64997c0cc103"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed3b0b8e-2dbe-44c9-8596-9d4316d1dc03"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 01e: HCAT allows you to view Details about your Table\nDESCRIBE TABLE perm_mng_table1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ace3f03f-fa7c-4484-a54b-b23a6e61c5a1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 01f: Table Column names and Data type and other\ndisplay(spark.catalog.listColumns(\"perm_mng_table1\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56c3046a-2cfb-4199-99c0-da701a5252ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 01g: HCAT shows your DLL statement\nSHOW CREATE TABLE perm_ext_table1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b175f20-595b-49c9-8319-cc15275adcd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 01h: Drop TempView and Confirm it is removed from Catalog\nspark.catalog.dropTempView(\"temp_view_1\")\ndisplay(spark.catalog.listTables)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f84a6438-93bd-4ccd-adfc-bd447225e853"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 01i: List Spark functions\ndisplay(spark.catalog.listFunctions())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"169a4c2b-c0f0-4e38-be94-5ab31557ad08"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01j: Spark Catalog can tell if if Object is 'cached' on not\nspark.read.format(\"json\").load(\"dbfs:/FileStore/tables/names1.json\").createOrReplaceTempView(\"temp_view1\")\nprint(spark.catalog.isCached('temp_view1'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f736b033-a4d0-4916-b4da-9b60719f40ba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01k: Next, we cache the table and ask Catalog if it is indeed Cached.  It is.\nspark.sql(\"cache table temp_view1\")\nprint(spark.catalog.isCached('temp_view1'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"053d2a4c-468e-4cd7-bcdc-0797dcc64e5d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 02: Catalyst Optimizier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"007f9722-13bc-4aa2-a5cc-314ab118b946"}}},{"cell_type":"code","source":["# Lab 02a: First, disable side effects\nspark.conf.set(\"spark.databricks.io.cache.enabled\", False)\nspark.conf.set(\"spark.sql.adaptive.enabled\", False)\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdd55a04-026f-4b5e-b3dc-c300d8003e04"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02b: First create some DataFrames and TempViews\nempDF = spark.read.parquet(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\ndeptDF = spark.read.parquet(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\n\nspark.read.parquet(\"dbfs:/FileStore/tables/emp_snappy.parquet/\").createOrReplaceTempView(\"emp_view\")\nspark.read.parquet(\"dbfs:/FileStore/tables/dept_snappy.parquet/\").createOrReplaceTempView(\"dept_view\")\n\ndisplay(empDF)\ndisplay(deptDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fdaf2b5-0de8-4856-b74a-bfcffb9ea00c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02c: Notice in first 2 Plans Catalyst does JOIN, then does FILTER\n#          But in Optimized Plan, Catalyst moves the FILTER before the JOIN for better Performance\nspark.sql(\"SELECT e.last_name, d.dept FROM emp_view e INNER JOIN dept_view d ON e.dept=d.dept WHERE e.dept = 301\").explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"093bec75-830b-4dc4-b471-06a81d26c5be"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02d: Notice the size = 46 MB for both files\ndisplay(dbutils.fs.ls(\"dbfs:/FileStore/tables/cops\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d64bae84-8267-417e-b459-9511687d81cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02e: Create TempViews \nspark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/cops_02_snappy.parquet\").createOrReplaceTempView(\"cops_view1\")\nspark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/cops_03_snappy.parquet\").createOrReplaceTempView(\"cops_view2\")\n\n# Here we use the 'explain' method so don't have to go to Spark UI 'SQL' tab. \n# Without a Hint, Catalyst uses Join Strategy = 'SortMergeJoin'\nspark.sql(\"SELECT a.Category, b.PdDistrict FROM cops_view1 a JOIN cops_view2 b ON a.Time = b.Time\").explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24a58c1b-bfb2-42e7-b902-bd3c35e7289b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02f: Go to Spark UI and go to SQL tab to confirm SortMergeJoin\ndisplay(spark.sql(\"SELECT a.Category, b.PdDistrict FROM cops_view1 a JOIN cops_view2 b ON a.Time = b.Time\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"116e0b08-6920-442a-8b80-a260762891dc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02g: Tell Catalyst it's OK to do BroadcastHashJoin on Table up to 50MB \nspark.sql(\"SET spark.sql.autoBroadcastJoinThreshold = 52428800\")\n\n# Lab 02h: I drop Hint and tell Catalyst to broadcast 'cops_view2' table\nspark.sql(\"SELECT /*+ BROADCAST(cops_view2) */ * FROM cops_view1 a JOIN cops_view2 b ON a.Time = b.Time\").explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c8ae908-7605-4cf6-b1e5-5fe67552a3a0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 02i:  From Spark UI, go to SQL tab and confirm Join Strategy = BroadcastHashJoin\nSELECT e.last_name, e.first_name, d.dept, e.dept, d.dept \nFROM emp_view e JOIN dept_view d \nON e.dept=d.dept WHERE e.dept = 301"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82c57c00-ed7d-46ef-9086-22fc7c861e2e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 03: Catalyst Column Pruning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5eec0344-1696-411d-9804-10f212028a45"}}},{"cell_type":"code","source":["# Lab 03b: Create 2 DataFrames (one as CSV, one as Delta)\nfrom pyspark.sql.types import StructType, StructField, StringType\n\npoliceSchema = StructType([StructField('IncidentNum', StringType(), True), StructField('Category', StringType(), True), StructField('Description', StringType(), True), StructField('DayOfWeek', StringType(), True), StructField('Date', StringType(), True), StructField('Time', StringType(), True), StructField('PdDistrict', StringType(), True),  StructField('Resolution', StringType(), True), StructField('Address', StringType(), True), StructField('X', StringType(), True), StructField('Y', StringType(), True), StructField('Loc', StringType(), True), StructField('PdId', StringType(), True)])\n\nCSVColPruneDF = spark.read.schema(policeSchema).csv(\"dbfs:/FileStore/tables/sfpd1/sf101\")\ndisplay(CSVColPruneDF)\n\nCSVColPruneDF.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta_colPrune/\")\nDeltaColPruneDF = spark.read.format(\"delta\").load(\"/tmp/delta_colPrune/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4269fac-e48c-443f-99cc-20846ef6f129"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 03c: Attempt Column Pruning on CSV File formats (1 of 2)\n# Go to Spark UI, SQL tab. \n\ndisplay(CSVColPruneDF.select(\"Category\", \"Description\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43e2d450-f708-49af-b8c7-a74e67afcd59"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 03d: Attempt Column Pruning on CSV File formats (2 of 2)\n# Go to Spark UI, SQL tab. \n\ndisplay(DeltaColPruneDF.select(\"Category\", \"Description\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c102e247-e81d-41b2-8a88-231ed2a232fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 04: Catalyst - Predicate Pushed Filters (WHERE clause)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d922515-58cf-458b-8d9f-f182ab721363"}}},{"cell_type":"code","source":["%scala\n// Here's JDBC driver and credentials to log into PostgreSQL\nimport org.apache.spark.sql.functions._\n\n// Configure this app to connect to a PostgreSQL database\nClass.forName(\"org.postgresql.Driver\")\n\nval connectionProperties = new java.util.Properties()\nconnectionProperties.put(\"user\", \"readonly\")\nconnectionProperties.put(\"password\", \"readonly\")\n\nval tableName = \"training.people_1m\"\nval jdbcUrl = \"jdbc:postgresql://server1.databricks.training:5432/training\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a7f0995-bfd4-430c-92be-286560cdb029"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Here we query PostgreSQL table\n// Goal: Have PostgreSQL only bring back Filter rows instead of entire table\n//       Spark only puts into Memory what it needs\n\nval df = spark.read\n  .jdbc(url=jdbcUrl, table=tableName, properties=connectionProperties) // Open a JDBC connect\n  .filter($\"id\" > 343517)                                             \ndisplay(df) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dad6aa7-333b-4c24-8de2-2eb69524932e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 05: Partition Pruning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc58a94e-4622-4518-bcd2-4366ceb3159c"}}},{"cell_type":"code","source":["%sql\n-- Lab 05a: Before we begin, we create Partitioned table 'cust_part'\n\n DROP TABLE IF EXISTS cust_part2;\n CREATE TABLE cust_part2 (id INT, name STRING) PARTITIONED BY (state STRING, city STRING);\n\n INSERT INTO cust_part2 PARTITION (state = 'CA', city = 'Fremont') VALUES (100, 'Al'); \n INSERT INTO cust_part2 PARTITION (state = 'CA', city = 'San Jose') VALUES (200, 'Bo'); \n INSERT INTO cust_part2 PARTITION (state = 'AZ', city = 'Peoria') VALUES (300, 'Cy');"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3752c1fd-4281-41ad-9d54-9a91381024b6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 05b: Confirm have 3 Directory Paths for Hive table 'cust_part'\n\ndisplay(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/cust_part2/state=AZ/\"))\ndisplay(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/cust_part2/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50cacaeb-b328-438b-8b1b-f1d849131e60"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 05c: Now, let's load the Partition Table into a DataFrame and confirm it has 3 Partitions (i.e.: 3 Directory paths)\n\npartitionDF = spark.table(\"cust_part2\")\ndisplay(partitionDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94488b68-99cc-41a8-8654-6f69c2ae0de2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- 05d:\nDESCRIBE cust_part2;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e58db2b2-07d8-497b-a0f0-fb592602c875"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 05e: Without WHERE clause using Partition column(s)\n-- Go to Spark UI and drill down to DETAILS to see how many files were read\n\nSELECT * FROM cust_part2;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b993450d-da15-436a-8ee5-723b80a187f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 05f: With Partitions by having WHERE clause with Partitioning columns  'city' and 'state'\n--  Notice it only scanned /user/hive/warehouse/cust_part/state=CA/city=Fremont\")) Partiton.\n\nSELECT city FROM cust_part2 WHERE city = 'Fremont' AND state = 'CA';"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c08b005-e282-499b-8a22-9ee78b9dbdfe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 05g: Another Partitioning Lab using a DataFrame instead of a Table\n\nautoDF = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/FileStore/tables/autos.csv\")\n\nautoDF.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"yearOfRegistration\").save(\"/tmp/autoDelta\")\nPartAutoDF = spark.read.format(\"delta\").load(\"/tmp/autoDelta\")\n\n# Check out the Partitioned Years\ndisplay(dbutils.fs.ls(\"dbfs:/tmp/autoDelta/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92e499aa-4d66-4773-8616-ab7bccdb6c9a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n# Lab 05h: Query Partitioned DataFrame\n# Partitioning colunn = 'yearOfRegistration'\ndisplay(PartAutoDF.select(PartAutoDF.brand, PartAutoDF.price, PartAutoDF.yearOfRegistration)\n        .where(PartAutoDF.yearOfRegistration.isin([2015,2016])))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1835828c-22d7-4cdb-b3e6-19502f3be893"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 06: Tungsten - Improved Memory Usage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35e101d6-b879-4217-a230-a53819daaf4f"}}},{"cell_type":"code","source":["# Do this first to prevent side effects\nspark.conf.set(\"spark.databricks.io.cache.enabled\", False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2054791b-4a83-4109-a730-5c703a3cc9aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 06a:\nimport org.apache.spark.storage.StorageLevel\n\n// Lab 14:  Improved Memory Usage in Cache \n// View 'Storage' tab in Spark UI to view RAM size consumed between RDD and DataFrame\n\nval rdd1 = sc.textFile(\"dbfs:/FileStore/tables/autos.csv\").map(_.split(\",\"))\nval rdd2 = rdd1.persist(StorageLevel.MEMORY_ONLY_SER)\nrdd2.count()\n\nval df1 = spark.read.option(\"header\" , \"true\").option(\"inferSchema\", \"true\").csv(\"dbfs:/FileStore/tables/autos.csv\")\nval df2 = df1.persist(StorageLevel.MEMORY_ONLY_SER)\ndf2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f5e0975-1271-4737-96ff-64e469e82b85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 06c: After execute, go to 'SQL' tab to view\nspark.range(1000).filter(\"id > 100\").selectExpr(\"sum(id)\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"934eb316-7512-4e55-8a98-6dd82925c4d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Module 08: Catalog, Catalyst, Tungsten"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7be29ff7-8ec8-4553-af9c-105f432d9408"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mod-08-Catalog-Catalyst-Tungsten!","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3262131266600993}},"nbformat":4,"nbformat_minor":0}
