{"cells":[{"cell_type":"markdown","source":["# Mod 01: Spark Basics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67e04e8c-03c5-4876-bba5-c67695b9bf4e"}}},{"cell_type":"markdown","source":["## Lab 01:  Using 'dbutils' command to view Files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a242ddd8-99d9-41ec-a4d6-27a154f047b4"}}},{"cell_type":"code","source":["# Lab 01a: Before we begin, confirm all files are loaded\n# Should have XX rows if you loaded everything correctly\ndisplay(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"049720b8-7e99-448f-ae38-7f652690fd72"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: View files under '/databricks-datasets/...'\ndisplay(dbutils.fs.ls(\"/databricks-datasets/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9802074-6e6f-4b61-94bd-5092548643b0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01c: To delete an individual file, type: \n# 'True' means command completed successfully (in other words, file was deleted)\n# dbutils.fs.rm(\"dbfs:/FileStore/tables/flights_abbr_1.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d6bb79c-accb-47e9-b9f1-cfa75e3e1cba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01d: How to drop a Folder and all it's contents\n# 'False' means command failed (Typically if occurs when Folder does not exist)\n# dbutils.fs.rm('FileStore/tables/', True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaa48cde-1f47-446c-96c5-62cd69bdd1e6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01e: View contents of a file \ndisplay(dbutils.fs.head(\"dbfs:/FileStore/tables/LifeExp.csv\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e748692-8885-4c86-9c4c-fb4708d15571"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 02: Language Interpreters (via '%')\n### Next 4 cells will show 4 different Intrepreters (Markdown, Python, Scala, SQL, SH)\n### Note if have Intrepreter defined, must be Row 1\n### Here is an example of a **Markdown** cell (1 of 5)\n### Double-click inside Cell to see the underlying code\n### Click anywhere outside Cell to view as Markdown"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82f08434-379d-4bc2-9150-589f38810186"}}},{"cell_type":"code","source":["%python\n#  Here's example of a Python cell (2 of 5)\n\n# Lab 01a: Create RDD from text file using 'sc.parallelize'\ndata = ([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\nbaseRDD = sc.parallelize(data)\nbaseRDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ffb650c-084a-405e-9540-cac1c9cb123c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n//  Here's example of a Scala cell (3 of 5)\nval baseRDD = sc.textFile(\"/FileStore/tables/lincoln.txt\")\nbaseRDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2f6222f-81a8-4856-bb73-b1678b04117d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Here's an example of a SQL cell (4 of 5)\nDROP TABLE IF EXISTS diamonds;\n\nCREATE TABLE diamonds\nUSING csv\nOPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b96ee55-b310-4f74-a04a-1e8124793ad0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- View contents of previous Cell\nSELECT * FROM diamonds"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d932d7a4-6130-447f-a921-a8c638aba704"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\n# Here's the Shell intrepreter (5 of 5)\nls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfe98b76-a35f-4ced-857d-04929eec1c8d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 03: Spark SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af848a30-f571-4ef5-9849-40f71c80baf2"}}},{"cell_type":"code","source":["%sql\n-- Lab 03a: Drop Hive Table, then create Hive Table and populate it\nDROP TABLE IF EXISTS flights_abbr;\n\nCREATE TABLE flights_abbr\nUSING CSV\nOPTIONS (path \"/FileStore/tables/header_flights_abbr.csv\", header \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a10be5f8-1645-404d-af95-d26304f4258b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- View contents of Hive Table\n\nSELECT * FROM flights_abbr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67bb2113-35ce-4064-bf86-8e42e2fe16ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Run Aggregate on Hive Table\n\nSELECT uniquecarrier, avg(depdelay) as AVGdelay FROM flights_abbr\nGROUP BY uniquecarrier\nORDER BY AVGdelay DESC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97201014-b00e-4c13-be4b-d03e9d8762e1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 04: DataFrame API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c170c0ee-2d17-42de-82b2-9ea5beed2364"}}},{"cell_type":"code","source":["df = spark.read.csv(\"/FileStore/tables/header_flights_abbr.csv\", header=True)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59679afd-4442-4ddd-a2f7-22d45690d06e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lab 05: Resilient Distributed Datasets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa350b06-f33f-483a-b23b-cfd5e39fffe9"}}},{"cell_type":"code","source":["%scala\n// Lab 05b: Query: Do WordCount using 3 separate statements using following Operations (flatMap, map, reduceByKey)\n\nval rdd1 = sc.textFile(\"/FileStore/tables/mary.txt\")\nval rdd2 = rdd1.flatMap(line => line.split(\" \"))\nval rdd3 = rdd2.map(word => (word, 1))\nval rdd4 = rdd3.reduceByKey((x,y) => x+y)\nrdd4.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0efb8166-be63-40c5-9e99-cd0a9a6e16b9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab of5b Query: Pipeline 3 Operations (flatMap, map, reduceByKey) into a single statement\n\nval rdd1 = sc.textFile(\"/FileStore/tables/mary.txt\")\nval rdd2 = rdd1.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((x,y) => x+y)\nrdd2.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5c0055f-f90a-400e-9d42-9953a8060c1c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n# Lab 05c: Distributed refers to breaking file into smaller Partitions so they can be run in Parallel\n#          To see # of Partitions, can use 'getNumPartitions' method.  Note this method only works on RDDs\nrdd1 = sc.textFile(\"/FileStore/tables/mary.txt\")\nrdd1.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80ad0574-8276-48e9-92ec-7168b831343b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n# Lab 05d:  To get # of Partitions for a DataFrame, use the 'rdd' method to convert DataFrame to rdd,\n#           then run the 'getNumPartitions'\ndf = spark.read.csv(\"/FileStore/tables/header_flights_abbr.csv\", header=True)\ndf.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53d003e2-3c0f-4ca7-a564-627023a77d47"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Module 01 - Basics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2588345-08ad-4ae0-80c8-5d85ad0ee244"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mod-01-Basics!","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3262131266600763}},"nbformat":4,"nbformat_minor":0}
