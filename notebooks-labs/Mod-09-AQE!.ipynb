{"cells":[{"cell_type":"markdown","source":["# Mod 09: Adaptive Query Execution"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f9b7cf2-ee40-413a-9433-158000d6ce82"}}},{"cell_type":"markdown","source":["### Lab 00: Setup Data sets for Labs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80bce65f-1d4a-4502-b8c6-68dba8e8e1c0"}}},{"cell_type":"code","source":["# Dynamic Partition Pruning Lab setup: Create Schema\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DecimalType\n\nfireSchema = StructType([StructField('IncidentNumber', IntegerType(), True),\nStructField('CallType', StringType(), True), StructField('ALSUnit', BooleanType(), True),\nStructField('CallTypeGroup', StringType(), True),StructField('UnitType', StringType(), True), \nStructField('NeighborhoodDistrict', StringType(), True), StructField('ReceivedDtTmTS', StringType(), True)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51dd22b6-9c75-414d-a965-5fd4e232f1f9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Dynamic Partition Pruning Lab setup: Apply Schema, convert to DF and create Non-Partitioned View and Display\n\nfireDF = spark.read.option(\"header\",\"true\").option(\"schema\",\"fireSchema\").option(\"sep\",\"\\t\").csv(\"/FileStore/tables/fire_callsX_comma.csv\")\ndisplay(fireDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdc94b82-efe6-4bea-ac54-9bb0455dc4a2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Dynamic Partition Pruning Lab setup: Create Non-Partitioned object\nfireDF.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/tables/fire_nonpart\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a8a4b15-d117-421c-ab7b-83b4c499b2b2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Dynamic Partition Pruning Lab setup: Create Partitioned object (Notice 'partitionBy' function)\nfireDF.write.mode(\"overwrite\").partitionBy(\"NeighborhoodDistrict\").parquet(\"dbfs:/FileStore/tables/fire_part\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd9e2e7e-0f7d-4bbd-a3b5-16d7c37f657d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Dynamic Partition Pruning Lab setup: Create Schema and Apply. Convert to Temp View\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DecimalType\nneighborSchema = StructType([StructField('Neighborhood', StringType(), True)])   \n\nspark.read.csv(\"dbfs:/FileStore/tables/neighbooddistricts.csv\", header=False, schema=neighborSchema).createOrReplaceTempView(\"neighbor_view\")\n\ndisplay(spark.sql(\"SELECT * FROM neighbor_view\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a92032db-42f6-44f9-ab1a-c4071a27612e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Dynamic Partition Pruning Lab setup:: Create DIM Parquet \n\ndimDF = spark.read.csv(\"dbfs:/FileStore/tables/neighbooddistricts.csv\", header=False, schema=neighborSchema)\ndimDF.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/tables/dim01\")\ndimDF1 = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dim01\")\ndisplay(dimDF1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58fb1e60-a37b-48c5-8511-a7cc73bf2ec4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Setup for Coalesce Shuffle Partitions: First create 2 objects we will be JOINing\n\nval txDF = spark.range(10 * 1000) // 10,000 rows\nval companyDF = spark.range(100)\n\ndef genRandomCompanyTx: ( () => (Int, String, String, String) ) = {\n  () => if ( scala.util.Random.nextInt(100) <= 10)\n  (1, //Very Big Company! (10%)\n  scala.util.Random.alphanumeric.take(5).mkString(\"\"),\n  scala.util.Random.alphanumeric.take(5).mkString(\"\"),\n  scala.util.Random.alphanumeric.take(5).mkString(\"\"))\n  else\n  (scala.util.Random.nextInt(100)%99,\n  scala.util.Random.alphanumeric.take(5).mkString(\"\"),\n  scala.util.Random.alphanumeric.take(5).mkString(\"\"),\n  scala.util.Random.alphanumeric.take(5).mkString(\"\"))\n}\n\ndef genCompanyLookup: ( (Int) => (String) ) = {\n  (id) => if (id ==1) (\"Very Big Company\") else (\"Company \"+id)\n  }\n\nval gen = org.apache.spark.sql.functions.udf(genRandomCompanyTx)\nval lkp = org.apache.spark.sql.functions.udf(genCompanyLookup)\n\nval sample = txDF.withColumn(\"newCol\", gen()).select(\"id\", \"newCol.*\").toDF(\"tx_id\", \"company_id\", \"field1\", \"field2\", \"field3\")\nsample.createOrReplaceTempView(\"sample\")\n\nval lookup = companyDF.withColumn(\"company\", lkp(companyDF(\"id\")))\nlookup.createOrReplaceTempView(\"lookup\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30bfd17e-80d4-408d-a700-0ea8b07954c3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Setup for Coalesce Shuffle Partitions: Write data to File System\n\nsample.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/sample\")\nlookup.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/lookup\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0eabb96c-d999-45e6-830e-b5077cefec29"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Setup for Coalesce Shuffle Partitions: Create Schemas for our 2 Dataframes\n\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n\nsampleSchema = StructType([StructField('tx_id', IntegerType(), True),\n                             StructField('company_id', IntegerType(), True),\n                             StructField('field1', StringType(), True),\n                             StructField('field2', StringType(), True),\n                             StructField('field3', StringType(), True)])\n\nlookupSchema = StructType([StructField('id', IntegerType(), True),\n                             StructField('company', StringType(), True)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41d87f78-3eee-43f5-93a9-6c1e8d5ed220"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Setup for Coalesce Shuffle Partitions: Read into Dataframes\n\nsampleDF = spark.read.load(\"dbfs:/FileStore/tables/sample/\", format = \"csv\", header = \"false\", schema = sampleSchema)\nlookupDF = spark.read.load(\"dbfs:/FileStore/tables/lookup/\", format = \"csv\", header = \"false\", schema = lookupSchema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7dcdf1b6-c1ba-4a35-9c30-427e13e4ed2b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Setup for Coalesce Shuffle Partitions: Convert CSV to Parquet\n\nsampleDF.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/sample_parq\")\nlookupDF.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/lookup_parq\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c340d727-0a44-47f7-b777-5faaf13fd365"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Setup for Coalesce Shuffle Partitions: Create 2 Dataframes\n\nsampleDF = spark.read.load(\"dbfs:/FileStore/tables/sample/\", format = \"parquet\", header = \"false\", schema = sampleSchema)\nlookupDF = spark.read.load(\"dbfs:/FileStore/tables/lookup/\", format = \"parquet\", header = \"false\", schema = lookupSchema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef1c3d3a-3264-48ab-aaa5-e7bba1d26aa4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Setup for Skew Partitions: Create Data objects\n\nimport scala.util.Random\nimport scala.math.BigDecimal\n\ncase class MakeModel(make: String, model: String)\n\ncase class T1(registration: String, make: String, model: String, engine_size: BigDecimal)\n\ncase class T2(make: String, model: String, engine_size: BigDecimal, sale_price: Double)\n\n    val makeModelSet: Seq[MakeModel] = Seq(\n      MakeModel(\"FORD\", \"FIESTA\")\n      , MakeModel(\"NISSAN\", \"QASHQAI\")\n      , MakeModel(\"HYUNDAI\", \"I20\")\n      , MakeModel(\"SUZUKI\", \"SWIFT\")\n      , MakeModel(\"MERCEDED_BENZ\", \"E CLASS\")\n      , MakeModel(\"VAUXHALL\", \"CORSA\")\n      , MakeModel(\"FIAT\", \"500\")\n      , MakeModel(\"SKODA\", \"OCTAVIA\")\n      , MakeModel(\"KIA\", \"RIO\")\n    )\n\n    def randomMakeModel(): MakeModel = {\n      val makeModelIndex = if (Random.nextBoolean()) 0 else Random.nextInt(makeModelSet.size)\n      makeModelSet(makeModelIndex)\n    }\n\n    def randomEngineSize() = BigDecimal(s\"1.${Random.nextInt(9)}\")\n\n    def randomRegistration(): String = s\"${Random.alphanumeric.take(7).mkString(\"\")}\"\n\n    def randomPrice() = 500 + Random.nextInt(5000)\n\n    def randomT1(): T1 = {\n      val makeModel = randomMakeModel()\n      T1(randomRegistration(), makeModel.make, makeModel.model, randomEngineSize())\n    }\n\n    def randomT2(): T2 = {\n      val makeModel = randomMakeModel()\n      T2(makeModel.make, makeModel.model, randomEngineSize(), randomPrice())\n    }\n\n    val t1 = Seq.fill(10000)(randomT1()).toDS()\n\n    val t2 = Seq.fill(100000)(randomT2()).toDS()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04fad7b8-edfc-43c0-aab1-37336082d715"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Setup for Skew Partitions:  Write to File\n\nt1.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/t1\")\nt2.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/t2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23da12d0-eb29-46af-8c83-e96a81284e6c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Mod 09 AQE: Adapative Query Execution"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d050350f-0c42-48ba-9b43-0752c5ca02b2"}}},{"cell_type":"markdown","source":["### Lab 01: Converting SortMergeJoin into a BroadcastHashJoin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f05fa34f-3d47-47ea-abd1-f12d0b8cf658"}}},{"cell_type":"code","source":["# Lab 01a: Know the Data (Emp and Dept)\ndisplay(spark.read.parquet(\"dbfs:/FileStore/tables/emp_snappy.parquet/\"))\ndisplay(spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dept_snappy.parquet/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"664b39bc-4550-4a7a-a263-0097c7bae2f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01b: Create DFs \nempDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/emp_snappy.parquet/\")\ndeptDF = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dept_snappy.parquet/\")\n\n# Convert DF into Spark Views          \nempDF.createOrReplaceTempView(\"emp_view\")\ndeptDF.createOrReplaceTempView(\"dept_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74afca02-1b6b-4522-ad7a-1895facb065e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01c: Turn off both BroadcastHashJoins and AQE\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)  \nspark.conf.set(\"spark.sql.adaptive.enabled\",False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"479cacd7-a693-49b4-936c-95033e4f52b5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01d: Do SortMergeJoin (Open any Job -> SQL -> Click on Description)\ndisplay(empDF.join(deptDF, \"dept\").select(\"last_name\", \"dept\", \"dept_name\").limit(4))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"319e480d-056b-4ec2-b89b-c38436c29aed"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 01e: Turn on BroadcastHashJoin and AQE and execute again.\n#         (Open any Job -> SQL -> Click on Description)\n#         Did Performance Improve based on Clock time compared to SortMergeJoin?\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)\nspark.conf.set(\"spark.sql.adaptive.enabled\",True)\n\ndisplay(empDF.join(deptDF, \"dept\").select(\"last_name\", \"dept\", \"dept_name\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cde9d07-df09-4eca-a676-7d9284353d5b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 02: Dynamic Coalesce Shuffle Partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43becbf6-ff03-4654-aa83-eafd3e81ff2c"}}},{"cell_type":"code","source":["# Lab 02a: Configure settings to not use AQE first\n\n# Disable BroadcastHashJoins to force a SortMergeJoin\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)  \n\n# Disable AQE\nspark.conf.set(\"spark.sql.adaptive.enabled\",False)\n\n# Force # of Shuffle Partitions = 50 as MAX. Default = 200\nspark.conf.set(\"spark.sql.shuffle.partitions\", 50)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92af7e4e-62c5-4e4b-90ea-778b21aa43f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02b: Know the Data\nspark.read.load(\"dbfs:/FileStore/tables/sample_parq/\", format = \"parquet\").createOrReplaceTempView(\"sample\")\nspark.read.load(\"dbfs:/FileStore/tables/lookup_parq/\", format = \"parquet\").createOrReplaceTempView(\"lookup\")\n\n# Notice 'company_id' = 1 will be a large Partition when compared to rest when JOIN on 'company_id' column\ndisplay(spark.sql(\"SELECT company_id, count(tx_id) as transactions FROM sample GROUP BY company_id ORDER BY transactions DESC LIMIT 10\"))\n\n# Here's other Table we will be JOINing\ndisplay(spark.sql(\"SELECT * FROM lookup ORDER BY id\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55430efc-b4cb-42e4-a92d-8de318e87162"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02c: Notice Hint to force SortMergeJoin (another Spark 3.x functionality)\n#          From Spark UI -> SQL -> Notice Lack of 'CustomShuffleReader' in DAG\n\ndisplay(spark.sql(\"SELECT /*+MERGE(sample, lookup)*/sample.tx_id, lookup.company FROM sample JOIN lookup ON sample.company_id = lookup.id\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b27481f7-9b74-4eda-a4b9-c5dda0bae964"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02d: Enable both AQE and Coalesce Partitions\nspark.conf.set(\"spark.sql.adaptive.enabled\", True)\n\n# When true and spark.sql.adaptive.enabled = true, Spark will coalesce contiguous shuffle partitions according to the target size \n# (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4979106-6418-4bc3-88d2-c0528c029e3e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 02e: Drop HINT to force SORT MERGE JOIN (instead of BroadcastHashJoin)\n# With AQE, it will Coalesce Shuffle Partitions\n# Spark UI -> SQL -> Look for 'CustomShuffleReader'\n\ndisplay(spark.sql(\"SELECT /*+MERGE(sample, lookup)*/ sample.tx_id, lookup.company, sample.field1 FROM sample JOIN lookup ON sample.company_id = lookup.id\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1ada9a6-5732-46d2-83b3-e85c578964eb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 03 AQE: Handling Skew Partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f14ee1c5-686a-483a-9b90-07d42d3c1260"}}},{"cell_type":"code","source":["%scala\n// Lab 03a: Load the Data\n\nval t1DF = spark.read.parquet(\"dbfs:/FileStore/tables/t1/\")\nval t2DF = spark.read.parquet(\"dbfs:/FileStore/tables/t2/\")\n\nt1DF.createOrReplaceTempView(\"t1_view\")\nt2DF.createOrReplaceTempView(\"t2_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"984ec2f2-839a-4a59-8d9c-4f0b352b7432"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 03b: Know the Data\nSELECT * FROM t1_view"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c567d10c-021a-4858-858f-c04419055e57"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 03c: Know the Data\nSELECT * FROM t2_view"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f0e23d3-dbca-4a42-b7d9-8203dc8bf80a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 03d: Notice Skew for 'Ford Fiesta'\nSELECT make, model, count(*) AS cnt FROM t2_view GROUP BY make, model ORDER BY cnt DESC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a34feea9-53bb-4782-bb3e-698130a594ea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 02:  View Spark UI to find Bottleneck.  It's a Skew Partition issue (2 minute query)\nimport org.apache.spark.sql.functions._\nimport scala.collection.Seq\n\n// We disable Broadcast join and AQE, then JOIN on 'make' and 'model'\n// In order to see our Skew happening, we need to suppress this behaviour\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\nspark.conf.set(\"spark.sql.adaptive.enabled\",false)\n\n// Skew eats up 2 Minutes in one of the Stages.  Ouch!!\ndisplay(t1DF.join(t2DF, Seq(\"make\", \"model\"))\n.filter(abs(t2DF(\"engine_size\") - t1DF(\"engine_size\")) <= BigDecimal(\"0.1\"))\n  .groupBy(\"registration\")\n  .agg(avg(\"sale_price\").as(\"average_price\")).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5776b7b4-868c-46ba-b46d-b9df74ed06f7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 03e: View Spark UI to find Bottleneck.  It's a Skew Partition issue (2 minute query)\n\nimport org.apache.spark.sql.functions._\n\n// We disable Broadcast join and AQE, then JOIN on 'make' and 'model'\n// In order to see our Skew happening, we need to suppress this behaviour\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\nspark.conf.set(\"spark.sql.adaptive.enabled\", false)\n\n// Skew eats up 2 Minutes in one of the Stages.  Ouch!!\ndisplay(t1DF.join(t2DF, Seq(\"make\", \"model\"))\n   .filter(abs(t2DF(\"engine_size\") - t1DF(\"engine_size\")) <= BigDecimal(\"0.1\"))\n  .groupBy(\"registration\")\n  .agg(avg(\"sale_price\").as(\"average_price\")).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b392d601-363d-4e50-a1e8-caf8f17c3de8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 03f: Let AQE and let it figure out the Skew problem and fix it automatically\n// First configure the Settings\n\nimport org.apache.spark.sql.functions._\n\n// We disable Broadcast join and enable AQE\n// In order to see our skew happening, we need to suppress this behaviour\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\nspark.conf.set(\"spark.sql.adaptive.enabled\", true)\n\n// I added this to see if it would work\n// Disable coalesce Partitions so Skew occurs\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", false)\nspark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n\n// A Partition is considered as skewed if its size is larger than this factor multiplying \n// The median partition size and also larger than //spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\nspark.conf.set(\"spark.sql.adaptive.skewedPartitionFactor\", 2)\n\n// A Partition is considered  skewed if its size in bytes is larger than this threshold and larger than spark.sql.adaptive.skewJoin\n// skewedPartitionFactor multiplying the median partition size. \n//Ideally this config should be set larger than spark.sql.adaptive.advisoryPartitionSizeInBytes.\n// Was 1KB, then 124 (1.95)\nspark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"1KB\")\n// Was 4KB then 512\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\",\"4KB\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"764d8c30-079a-416e-bea9-705df0136c23"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Lab 03g: Solution: Let AQE figure out the Skew problem and fix it automatically\n// Keep tweaking above settings to get better Performance here\n\ndisplay(t1DF.join(t2DF, Seq(\"make\", \"model\"))\n.filter(abs(t2DF(\"engine_size\") - t1DF(\"engine_size\")) <= BigDecimal(\"0.1\"))\n  .groupBy(\"registration\")\n  .agg(avg(\"sale_price\").as(\"average_price\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"503b53ca-7f97-40f3-a742-a810c48fb20f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 04: Dynamic Partition Pruning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"394197e8-dc17-45a1-9479-797cec1d623e"}}},{"cell_type":"code","source":["# Lab 04a: First, view Directory Partitions Folders (Partitioned by 'NeighborhoodDistrict')\ndisplay(dbutils.fs.ls(\"dbfs:/FileStore/tables/fire_part/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e54d71cb-c854-4ca4-aa89-4ded3ce6c3a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 04b: Load and View 'firePartition_view' \nspark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/fire_part\").createOrReplaceTempView(\"firePartition_view\")\ndisplay(spark.sql(\"SELECT * FROM firePartition_view LIMIT 5\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e97e98ba-ff26-448b-b6db-35f28329125e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 04c: Load and View 'fireNonPart_view' (same Output as firePartition_view)\nspark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/fire_nonpart\").createOrReplaceTempView(\"fireNonPart_view\")\ndisplay(spark.sql(\"SELECT * FROM fireNonPart_view LIMIT 5\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e4953a6-b16d-4acc-9609-5931cf787a86"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lab 04d: Create DIM object. Convert to Temp View\ndimDF1 = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/dim01\")\ndimDF1.createOrReplaceTempView(\"neighbor_view\")\ndisplay(dimDF1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9909fb31-2a18-4f17-85b6-23ef3c8027a1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Lab 04e: First, ensure Defaults are enabled for AQE\n\nspark.conf.set(\"spark.sql.adaptive.enabled\",True)\nspark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly\", True)\nspark.conf.set(\"spark.sql.cbo.enabled\", True)\nspark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0a9e830-6d6e-42ef-9ff2-97beda842dc8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 04f: Dynamic Pruning (and FILTER) is taking place on large FACT table (firePartition_view)\n--          Even though the WHERE clause only defines filtering on DIM table (neighbor_view)\n\nSELECT fact.NeighborhoodDistrict, dim.Neighborhood, fact.UnitType \nFROM neighbor_view AS dim INNER JOIN firePartition_view AS fact \nON dim.Neighborhood = fact.NeighborhoodDistrict\nWHERE dim.Neighborhood IN ('Golden Gate Park', 'Twin Peaks')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31821c51-ef2c-4326-a154-79dbbd40c94b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Lab 04g: Dynamic Pruning won't happen if Fact Table NOT Partitioned\n\nSELECT fact.NeighborhoodDistrict, dim.Neighborhood, fact.UnitType \nFROM neighbor_view AS dim INNER JOIN fireNonPart_view AS fact \nON dim.Neighborhood = fact.NeighborhoodDistrict\nWHERE dim.Neighborhood IN ('Golden Gate Park', 'Twin Peaks')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2616a45-0a83-4f73-a2f6-3db089ea1adf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Module 09: AQE (Adapative Query Execution)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46073255-9095-4c6a-bc74-0f6092d51d51"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mod-09-AQE!","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3262131266600612}},"nbformat":4,"nbformat_minor":0}
