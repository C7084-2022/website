{"cells":[{"cell_type":"code","source":["# Lab 00a: Before we begin, confirm all files are loaded\ndisplay(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e8290fd-76a3-4429-8254-87d7484ca2e0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Mod 06a: Streaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d90fe14f-c82e-4a00-b902-1456a3d295df"}}},{"cell_type":"code","source":["# https://hackersandslackers.com/structured-streaming-in-pyspark/\n\n# https://medium.com/expedia-group-tech/apache-spark-structured-streaming-checkpoints-and-triggers-4-of-6-b6f15d5cfd8d\n\n#dbutils.fs.rm(\"dbfs:/FileStore/tables/sfpd/\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2838d966-df72-48b9-a57d-78f96d0485fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 01: Aggregate using BATCH"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37177c56-d845-41bd-9cec-488c2c4d878f"}}},{"cell_type":"code","source":["# Here's the files we'll be Streaming in\ndbutils.fs.ls(\"dbfs:/FileStore/tables/stream1\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c2b55bd-6207-4fbb-87dd-ed2ba8be3628"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# View contents of 1 of the 20 JSON files\ndisplay(spark.read.json(\"dbfs:/FileStore/tables/stream1/1.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e3c4624-ebb1-44d1-a454-329c7a972d22"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Explicitly set schema (This is mandatory for Streaming)\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField\n\nDDL_schema = StructType([ StructField(\"time\", StringType(), True),\n                      StructField(\"customer\", StringType(), True),\n                      StructField(\"action\", StringType(), True),\n                      StructField(\"device\", StringType(), True)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03033c05-c0b7-4c25-9bc2-2b6769b427c1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create DataFrame representing data in the JSON files\nstaticDF = (spark.read.schema(DDL_schema)\n                 .option(\"header\", True)\n                 .json(\"dbfs:/FileStore/tables/stream1/\").na.drop())\n\ndisplay(staticDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f3223ea-f4ea-408d-ae3e-4f663a1781b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Aggregate number of actions\nactionsDF = (staticDF.groupBy(staticDF.action).count())\n\n# Create Temp table named 'iot_action_counts'\nactionsDF.createOrReplaceTempView(\"iot_action_counts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52abab33-c39c-41f5-81ad-9b101724af82"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT action, sum(count) as total_count \nFROM iot_action_counts GROUP BY action"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24a7c183-ce67-4eed-8f9d-7ae30a29eed2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Lab 02: Aggregate using STREAMING"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54267463-c8b7-4874-b1d3-2b41292b36d8"}}},{"cell_type":"code","source":["# Our data isn't being created in real time, so we'll have to use a trick to emulate streaming conditions. \n# Instead of streaming data as it comes in, we can load each of our JSON files one at a time.  \n# That's what option() is doing: we're setting the maxFilesPerTrigger option to 1, \n# which means only a single JSON file will be streamed at a time. This will allow us to see the #data as it streams in!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8d08df7-177b-4f18-9fdf-5f99bb438ebc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create streaming equivalent of `staticDF` using .readStream\n# Only read in 1 file per Trigger as defined in 'writeStream' (in later Cell)\nstreamReadDF = (spark.readStream\n    .schema(DDL_schema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .json(\"dbfs:/FileStore/tables/stream1/\"))\n\n# Count # of 'action' in Streaming DataFrame\nstreamingActionCountsDF = (streamReadDF.groupBy(streamReadDF.action).count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b287d1b8-12c3-4fe2-8087-0fe809a9b5da"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Is `streamingActionCountsDF` actually streaming?\nstreamingActionCountsDF.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7aed49ab-5f2a-4362-82d1-97be4d8237d5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# To start 'readStream', normally have a 'writeStream' followed by 'start()'.\n# But can issue an Action like 'display' to view the Output from 'readStream' \n# Let's do it to see Streaming in Action as each of the 20 files get Aggregated\n# When 'counts' match Cell 12, click on 'Cancel' hotlink to reset\ndisplay(streamingActionCountsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b977c3dd-377f-4412-90ad-ca5edf069595"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Here's the 'writeStream'. Notice we point to the 'readStream' DataFrame named = 'streamingActionCountsDF'\n# We are writing the Output to a TempView named = 'counts'\n# And we want 'outputMode' = Complete which means we'll get full count as each JSON files streams in\n# We set a Trigger so each file will Stream in every 5 seconds\n# And we set a Checkpoint for Fault Tolerance purposes\n# The 'start' kicks off the Streaming\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n\n# View stream in 5-second micro-batches\nquery = (streamingActionCountsDF\n    .writeStream\n    .format(\"memory\")\n    .queryName(\"counts\")\n    .outputMode(\"complete\")\n    .trigger(processingTime = \"5 seconds\")\n    .option(\"checkpointLocation\", \"/tmp/checkpoint/\")\n    .start())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73fa2595-4fed-45c9-895b-76801efc4c25"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# While above Cell is running, Is streaming query still Active?\nquery.status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70f09a47-648f-4b35-9c4f-636bb9727158"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Click this every 5 seconds to view the updated counts\nSELECT * FROM counts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b83fd701-98c9-4216-a528-b4663719df1b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Stop Streaming Job\nquery.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c3baa49-8c2d-4643-b3f1-dbfb461357b6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Confirm Checkpoint directory is populated\ndbutils.fs.ls(\"tmp/checkpoint/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2679e45d-e17c-48e6-a0b0-8ec2d7f107f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Confirm have Offset for every File that was Micro-batched\ndbutils.fs.ls(\"dbfs:/tmp/checkpoint/offsets/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e6bc52-5ec8-4543-8ca2-e4e0f0ffb8e1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Clean up the Checkpoint Files\ndbutils.fs.rm(\"dbfs:/tmp/checkpoint/\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9878b0a-8d94-4607-a7f0-55e273683fe8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# End of Module 06a-Streaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7236cbaf-32de-4dee-8bba-d7a96734191f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mod-06-Streaming!","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3262131266600969}},"nbformat":4,"nbformat_minor":0}
